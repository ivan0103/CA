{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Speak now!\n",
      "Recording complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanvirovski/.pyenv/versions/3.8.18/lib/python3.8/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "/Users/ivanvirovski/.pyenv/versions/3.8.18/lib/python3.8/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper Transcription:  Hello my name is Bann and I'm doing always been model for chap and chap and chapa chupin\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import whisper\n",
    "import os\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"\n",
    "    Captures audio from the microphone and returns it as an `AudioData` object.\n",
    "    \n",
    "    - Uses `speech_recognition.Recognizer` for capturing audio.\n",
    "    - Adjusts for ambient noise before recording.\n",
    "    - Listens for the user's speech and returns the recorded audio.\n",
    "    \n",
    "    Returns:\n",
    "        sr.AudioData: The recorded audio data.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        print(\"Recording... Speak now!\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Recording complete.\")\n",
    "        return audio\n",
    "\n",
    "def speech_to_text_whisper(audio):\n",
    "    \"\"\"\n",
    "    Converts recorded audio into text using OpenAI's Whisper model.\n",
    "    \n",
    "    - Loads the Whisper model (`medium`).\n",
    "    - Saves the recorded audio as a temporary WAV file.\n",
    "    - Uses Whisper to transcribe the saved audio.\n",
    "    - Deletes the temporary file after transcription.\n",
    "    \n",
    "    Args:\n",
    "        audio (sr.AudioData): The recorded audio data.\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcribed text.\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"medium\")\n",
    "    \n",
    "    temp_filename = \"temp.wav\"\n",
    "    with open(temp_filename, \"wb\") as f:\n",
    "        f.write(audio.get_wav_data())\n",
    "    \n",
    "    result = model.transcribe(temp_filename)\n",
    "    \n",
    "    os.remove(temp_filename)\n",
    "    return result[\"text\"]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main execution function.\n",
    "    \n",
    "    - Records audio from the microphone.\n",
    "    - Transcribes the recorded audio using Whisper.\n",
    "    - Prints the transcribed text or an error message if transcription fails.\n",
    "    \"\"\"\n",
    "    audio = record_audio()\n",
    "    \n",
    "    try:\n",
    "        whisper_text = speech_to_text_whisper(audio)\n",
    "        print(\"Whisper Transcription:\", whisper_text)\n",
    "    except Exception as e:\n",
    "        print(\"Whisper failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
